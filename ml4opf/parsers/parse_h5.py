import torch, numpy as np, h5py

from typing import Union, Optional
from pathlib import Path
from fnmatch import fnmatch

from ml4opf import info, debug, warn

ParsedHDF5Dict = dict[str, Union[torch.Tensor, np.ndarray, np.str_]]
H5Object = Union[h5py.Group, h5py.Dataset]


class H5Parser:
    """
    Parser for HDF5 files generated by AI4OPT/OPFGenerator.
    """

    def __init__(self, input_path: Union[str, Path], data_path: Union[str, Path]):
        """Initialize the parser by validating and setting the path."""
        self.input_path = self.validate_path(input_path)
        self.data_path = self.validate_path(data_path)

    def validate_path(self, path: Union[str, Path]) -> Path:
        """Validate the path to the HDF5 file."""
        path_obj = Path(path).resolve()
        assert path_obj.exists(), f"HDF5 Path {path_obj} does not exist."
        assert path_obj.is_file(), f"HDF5 Path {path_obj} is not a file."

        return path_obj

    def parse(
        self,
        parse_only: Optional[list[str]] = None,
        train_set_size: Optional[int] = None,
        feasible_by: Optional[dict[str, str]] = None,
        make_test_set: bool = False,
        test_set_size: int = 5000,
        total_load_range: tuple[Optional[float], Optional[float]] = (None, None),
        convert_to_float32: bool = True,
        sanity_check: bool = True,
    ) -> Union[ParsedHDF5Dict, tuple[ParsedHDF5Dict, ParsedHDF5Dict]]:
        """Parse the HDF5 file.

        Args:
            parse_only (list[str], optional): A list of strings of the form `solution/primal/pg` where `solution` is the group, `primal` is the subgroup, and `pg` is the dataset from the HDF5 file. Defaults to None.
            train_set_size (int, optional): The number of samples to keep, not including the test set size. Defaults to None.
            feasible_by (dict[str, str], optional): A dictionary of key-value pairs to filter by. Defaults to None.
            make_test_set (bool, optional): If True, return a tuple of two dictionaries (training set, test set). Defaults to False.
            test_set_size (int, optional): The number of points to use for the test set. Defaults to 5000.
            total_load_range (tuple[Optional[float], Optional[float]], optional): Range of total load values to use. Defaults to (None, None).
            convert_to_float32 (bool, optional): If True, convert all float64 data to torch.float32. Defaults to True.
            sanity_check (bool, optional): If True, perform sanity checks on the data. Defaults to True.

        Returns:
            dict: Flattened dictionary of HDF5 data with PyTorch tensors for numerical data and NumPy arrays for string/object data.

        If `make_test_set` is True, then this function will return a tuple of two dictionaries. The first dictionary is the
        training set and the second dictionary is the test set. The test set is a random 10% sample of the training set.

        This parser will return a single-level dictionary where the keys are in the form
        of `solution/primal/pg` where `solution` is the group, `primal` is the subgroup,
        and `pg` is the dataset from the HDF5 file. The values are PyTorch tensors. This
        parser uses `h5py.File.visititems` to iterate over the HDF5 file quickly.
        """
        dat: ParsedHDF5Dict = dict()

        def read_direct(dataset: h5py.Dataset):
            arr = np.empty(dataset.shape, dtype=dataset.dtype)

            if set(dataset.shape) == {0}:
                pass  # pragma: no cover
            else:
                dataset.read_direct(arr)

            if arr.ndim == 2:
                if arr.shape[0] == 1:
                    debug(f"Converting {dataset.name}: {arr} with shape (1, {arr.shape[1]}) to shape ({arr.shape[1],}).")
                    arr = arr.squeeze(0)
                elif arr.shape[1] == 1:
                    debug(f"Converting {dataset.name}: {arr} with shape ({arr.shape[0]}, 1) to shape ({arr.shape[0],}).")
                    arr = arr.squeeze(1)

            return arr

        parse_only: set[str] = set(parse_only) if parse_only is not None else None

        if parse_only and feasible_by is not None:
            for key in feasible_by:
                parse_only.add(key)
                debug(f"Added {key} to parse_only since feasible_only is True.")

        if parse_only and (total_load_range[0] is not None or total_load_range[1] is not None):
            parse_only.add("input/pd")
            debug("Added input/pd to parse_only since total_load_range is not (None, None).")

        if parse_only:
            info(f"Only parsing the following keys: {parse_only}.")

        parse_only_match_strs = [k for k in parse_only if any(c in k for c in "*?![]")] if parse_only else None

        def should_skip(name: str, obj: H5Object):
            if sanity_check and "*?![]" in name:
                raise ValueError(
                    f"Found key {name}: fnmatch characters *?![] are not allowed in HDF5 keys."
                )  # pragma: no cover

            if isinstance(obj, h5py.Group):
                return True

            if parse_only is not None and name not in parse_only:
                if parse_only_match_strs is not None:
                    for glob in parse_only_match_strs:
                        if fnmatch(name, glob):
                            return False
                return True
            else:
                return False

        def make_key(name: str, mode: str) -> str:
            if mode == "data":
                return name
            elif mode == "input":
                if name in ["meta", "data"]:
                    return name  # will be skipped anyway since these are groups
                elif name.startswith("meta/"):
                    # NOTE: when parsing input file, we keep input meta separate from data meta
                    #       by prepending "input/" e.g. "meta/config" -> "input/meta/config"
                    return "input/" + name
                elif name.startswith("data/"):
                    # NOTE: when parsing input file, we keep input data separate from solution data
                    #       by replacing "data/" with "input/" e.g. "data/pd" -> "input/pd"
                    return name.replace("data/", "input/", 1)
                else:
                    raise ValueError(f"Invalid key found in input file: {name}")  # pragma: no cover
            else:
                raise ValueError(f"Invalid mode: {mode}")  # pragma: no cover

        def store(name: str, obj: H5Object, mode: str):
            new_name = make_key(name, mode)
            if should_skip(new_name, obj):
                return
            elif isinstance(obj, h5py.Dataset):
                if (
                    obj.dtype.kind in "fiucb"
                ):  # floating-point, signed integer, unsigned integer, complex floating-point, boolean
                    dat[new_name] = torch.from_numpy(read_direct(obj))
                elif obj.dtype.kind in "SO":  # (byte-)string/object
                    dat[new_name] = read_direct(obj).astype(str)
                    if dat[new_name].ndim == 0:
                        debug(f"Converting {name}: {dat[new_name]} with shape () to shape (1,).")
                        dat[new_name] = dat[new_name][()]
                else:
                    raise ValueError("Unknown dtype: {} in name {}".format(obj.dtype, name))  # pragma: no cover
            else:
                raise ValueError("Unknown type: {} in name {}".format(type(obj), name))  # pragma: no cover

        debug("Opening HDF5 data file")
        with h5py.File(self.data_path, "r") as f:
            f.visititems(lambda name, obj: store(name, obj, "data"))
        debug("Closed HDF5 data file")

        debug("Opening HDF5 input file")
        with h5py.File(self.input_path, "r") as f:
            f.visititems(lambda name, obj: store(name, obj, "input"))
        debug("Closed HDF5 input file")

        if sanity_check:
            shapes = [
                (k, v.shape[0])
                for k, v in dat.items()
                if isinstance(v, (torch.Tensor, np.ndarray)) and not k.startswith(("meta/config", "meta/ref"))
            ]
            assert (
                len(set([s[1] for s in shapes])) == 1
            ), f"Batch dimensions are not all the same in data tensors!: {shapes}"

        if convert_to_float32:
            self.convert_to_float32(dat)

        if feasible_by is not None:
            self.remove_infeasible_points(dat, feasible_by=feasible_by)

        if total_load_range[0] is not None or total_load_range[1] is not None:
            self.filter_by_total_load(dat, total_load_range)

        if make_test_set:
            test = self.extract_test_set(dat, test_set_size)

        if train_set_size is not None:
            self.keep_n_samples(dat, train_set_size)

        if make_test_set:
            info(
                f"Parsed {H5Parser.get_n_samples(dat, sanity_check=sanity_check)} training samples and "
                f"{H5Parser.get_n_samples(test, sanity_check=sanity_check)} test samples from {self.data_path}."
            )
        else:
            info(f"Parsed {H5Parser.get_n_samples(dat, sanity_check=sanity_check)} samples from {self.data_path}.")

        return dat if not make_test_set else (dat, test)

    @staticmethod
    def get_n_samples(dat: ParsedHDF5Dict, sanity_check: bool = False):
        n = None
        for key in dat:
            if dat[key].ndim != 0:
                if sanity_check and n is not None:
                    assert n == dat[key].shape[0], f"Mismatch in shape from {key_} to {key}: {n} != {dat[key].shape[0]}"

                n = dat[key].shape[0]

                if sanity_check:
                    key_ = key
                else:
                    break
        if n is None:
            raise ValueError("Could not determine number of samples from data dictionary.")
        return n

    @staticmethod
    def make_tree(dat: ParsedHDF5Dict, delimiter: str = "/"):
        """Convert a flat dictionary to a tree.
        Note that the keys of `dat` must have a tree structure where data is only at the leaves.
        Assumes keys are delimited by "/", i.e. "solution/primal/pg".

        Args:
            dat (dict): Flat dictionary of data.
            delimiter (str, optional): Delimiter to use for splitting keys. Defaults to "/".

        Returns:
            dict: Tree dictionary of data from `dat`.
        """
        tree = dict()

        def r_correct_shape(d: dict, ret: dict):
            for k in list(d.keys()):
                if delimiter in k:
                    k1, k2 = k.split(delimiter, 1)
                    if k1 not in ret:
                        ret[k1] = dict()
                    r_correct_shape({k2: d[k]}, ret[k1])
                    del d[k]
                else:
                    ret[k] = d[k]

        r_correct_shape(dat, tree)

        return tree

    @staticmethod
    def convert_to_float32(dat: ParsedHDF5Dict):
        """Convert all float64 data to float32 in-place."""
        for k, v in dat.items():
            if isinstance(v, torch.Tensor) and v.dtype == torch.float64:
                dat[k] = v.to(torch.float32)

    @staticmethod
    def apply_mask(dat: ParsedHDF5Dict, mask: torch.Tensor):
        """Apply a mask to the data dictionary in-place."""
        if mask.ndim == 1 and mask.dtype == torch.bool:
            pass
        elif mask.ndim == 2:
            assert mask.shape[1] == 1, f"2D mask shape {mask.shape} is not (n, 1)."
            mask = mask.squeeze(1).to(torch.bool)
        elif mask.ndim == 1 and set(mask.unique().tolist()).issubset({0, 1}):  # pragma: no cover
            mask = mask.to(torch.bool)
        else:
            raise ValueError(f"Invalid mask shape: {mask.shape} or dtype: {mask.dtype}")

        for key in dat:
            if dat[key].ndim == 0:
                pass
            elif dat[key].ndim == 1:
                dat[key] = dat[key][mask]
            elif dat[key].ndim == 2:
                dat[key] = dat[key][mask, :]
            else:
                raise ValueError(
                    "Unexpected shape length: {} from key {}".format(dat[key].shape, key)
                )  # pragma: no cover

        shapes = [(k, v.shape[0]) for k, v in dat.items() if v.ndim != 0]
        assert (
            len(set([s[1] for s in shapes])) == 1
        ), f"Batch dimensions are not all the same in data tensors!: {shapes}"

    @staticmethod
    def remove_infeasible_points(dat: ParsedHDF5Dict, feasible_by: dict[str, str]):
        """Remove infeasible points from the data dictionary in-place

        Args:
            dat (ParsedHDF5Dict): Dictionary of parsed data from .parse().
            feasible_by (dict[str, str]): Dictionary of key-value pairs to filter by.
        """
        mask = torch.ones(H5Parser.get_n_samples(dat), dtype=torch.bool)
        for key, value in feasible_by.items():
            mask &= dat[key] == value

        n_points_excluded = mask.logical_not().sum()

        if n_points_excluded > 0:
            info(f"Deleting {n_points_excluded} points of {len(mask)} total points due to non-feasible solutions.")

        H5Parser.apply_mask(dat, mask)

    @staticmethod
    def filter_by_total_load(dat: ParsedHDF5Dict, bounds: tuple[Optional[float], Optional[float]] = (None, None)):
        """Filter data by total load.

        Args:
            dat (ParsedHDF5Dict): Dictionary of parsed data from `.parse()`.
            upper (float, optional): Upper bound on total load. Defaults to None.
            lower (float, optional): Lower bound on total load. Defaults to None.
        """
        lower, upper = bounds

        if upper is None and lower is None:
            return

        total_load = dat["input/pd"].sum(dim=1)
        mask = torch.ones((len(total_load),), dtype=torch.bool)
        if upper is not None:
            mask &= total_load <= upper
        if lower is not None:
            mask &= total_load >= lower

        n_points_excluded = mask.logical_not().sum()
        if n_points_excluded > 0:
            info(
                f"Deleting {n_points_excluded} points of {len(mask)} total points due to total load outside of [{lower}, {upper}] range."
            )

        H5Parser.apply_mask(dat, mask)

    @staticmethod
    def extract_test_set(dat: ParsedHDF5Dict, test_set_size: int = 5000, seed: Optional[int] = 42) -> ParsedHDF5Dict:
        """Extract a test set from the data dictionary in-place.

        Args:
            dat (ParsedHDF5Dict): Dictionary of parsed data from `.parse()`.
            test_set_size (int, optional): Size of the test set. Defaults to 5000.
            seed (Optional[int], optional): Seed for random number generator. Defaults to 42.

        Returns:
            ParsedHDF5Dict: Dictionary of test portion of the data.

        After this function is called, the `dat` dictionary will contain
        only training data (to be split downstream into training / validation)
        and the returned dictionary will contain only test data.
        """
        n = H5Parser.get_n_samples(dat)
        assert (
            n >= test_set_size
        ), f"Test set size {test_set_size} is larger than the total number of points {n} after all filters."

        test_idx = torch.zeros((n,), dtype=bool)
        if seed is not None:
            rng = torch.Generator().manual_seed(seed)
            test_idx[torch.randperm(n, generator=rng)[:test_set_size]] = True
        else:
            test_idx[torch.randperm(n)[:test_set_size]] = True

        test: ParsedHDF5Dict = dict()
        for key in dat:
            if dat[key].ndim == 0:
                test[key] = dat[key]  # NOTE: any 0-dim (e.g. meta/config) data is copied without indexing.
            elif dat[key].ndim == 1:
                test[key] = dat[key][test_idx]
                dat[key] = dat[key][test_idx.logical_not()]
            elif dat[key].ndim == 2:
                test[key] = dat[key][test_idx, :]
                dat[key] = dat[key][test_idx.logical_not(), :]
            else:
                raise ValueError(
                    "Unexpected shape length: {} from key {}".format(dat[key].shape, key)
                )  # pragma: no cover

        dshapes = [(k, v.shape[0]) for k, v in dat.items() if v.ndim != 0]
        assert (
            len(set([s[1] for s in dshapes])) == 1
        ), f"Batch dimensions are not all the same in train data tensors!: {dshapes}"

        tshapes = [(k, v.shape[0]) for k, v in test.items() if v.ndim != 0]
        assert (
            len(set([s[1] for s in tshapes])) == 1
        ), f"Batch dimensions are not all the same in test data tensors!: {tshapes}"

        n_train = list(set([s[1] for s in dshapes]))[0]
        n_test = list(set([s[1] for s in tshapes]))[0]
        assert n_train + n_test == n, f"Lost some samples in the test set extraction! {n} != {n_train} + {n_test}"

        debug(f"Extracted {len(test[tshapes[0][0]])} points for testing.")

        return test

    @staticmethod
    def keep_n_samples(dat: ParsedHDF5Dict, n: int, seed: Optional[int] = 42) -> ParsedHDF5Dict:
        """Keep only `n` samples from the data dictionary in-place."""
        total = H5Parser.get_n_samples(dat)
        if n >= total:
            warn(
                f"Requested to keep {n} samples, but there are only {total} samples in the data dictionary. Keeping all samples."
            )
            return

        keep_idx = torch.zeros((total,), dtype=torch.bool)
        if seed is not None:
            rng = torch.Generator().manual_seed(seed)
            keep_idx[torch.randperm(len(keep_idx), generator=rng)[:n]] = True
        else:
            keep_idx[torch.randperm(len(keep_idx))[:n]] = True

        debug(f"Keeping {n} points of {total} total points.")

        H5Parser.apply_mask(dat, keep_idx)
